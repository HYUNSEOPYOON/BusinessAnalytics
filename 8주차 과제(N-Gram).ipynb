{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "categories = ['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n",
    "# 카테고리 설정\n",
    "\n",
    "# train set, test set 설정\n",
    "newsgroups_train = fetch_20newsgroups(subset='train',remove=('headers', 'footers', 'quotes'),categories=categories)\n",
    "\n",
    "newsgroups_test = fetch_20newsgroups(subset='test', remove=('headers', 'footers', 'quotes'),categories=categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set size: 2034\n",
      "test set size: 1353\n",
      "selected categories: ['alt.atheism', 'comp.graphics', 'sci.space', 'talk.religion.misc']\n",
      "train labels: {0, 1, 2, 3}\n"
     ]
    }
   ],
   "source": [
    "print('train set size:', len(newsgroups_train.data))\n",
    "print('test set size:', len(newsgroups_test.data))\n",
    "print('selected categories:', newsgroups_train.target_names)\n",
    "print('train labels:', set(newsgroups_train.target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 3, 2, ..., 1, 0, 1], dtype=int64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##Train set text samples: Hi,\n",
      "\n",
      "I've noticed that if you only save a model (with all your mapping planes\n",
      "positioned carefully) to a .3DS file that when you reload it after restarting\n",
      "3DS, they are given a default position and orientation.  But if you save\n",
      "to a .PRJ file their positions/orientation are preserved.  Does anyone\n",
      "know why this information is not stored in the .3DS file?  Nothing is\n",
      "explicitly said in the manual about saving texture rules in the .PRJ file. \n",
      "I'd like to be able to read the texture rule information, does anyone have \n",
      "the format for the .PRJ file?\n",
      "\n",
      "Is the .CEL file format available from somewhere?\n",
      "\n",
      "Rych\n",
      "##Train set label samples: 1\n",
      "##Test set text samples: TRry the SKywatch project in  Arizona.\n",
      "##Test set label samples: 2\n"
     ]
    }
   ],
   "source": [
    "print('##Train set text samples:', newsgroups_train.data[0]) #train data 첫번째 요소\n",
    "print('##Train set label samples:', newsgroups_train.target[0]) # 첫번째 라벨\n",
    "print('##Test set text samples:', newsgroups_test.data[0]) # test data 첫번째 요소\n",
    "print('##Test set label samples:', newsgroups_test.target[0]) # 첫번째 라벨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = newsgroups_train.data\n",
    "X_test = newsgroups_test.data\n",
    "y_train = newsgroups_train.target\n",
    "y_test = newsgroups_test.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "cachedStopWords = stopwords.words(\"english\") # 불용어 리스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2034, 11483)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(token_pattern= \"[a-zA-Z']{3,}\", decode_error ='ignore', lowercase=True, stop_words = stopwords.words('english'), max_df=0.5,min_df=2).fit(X_train)\n",
    "# tfidf 설정\n",
    "X_train_tfidf = tfidf.transform(X_train) # x_train을 이용하여 tfidf 구함\n",
    "X_test_tfidf = tfidf.transform(X_test) # x_test를 이용하여 tfidf 구함\n",
    "\n",
    "print(X_train_tfidf.shape)  # tfidf의 차원"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set score: 0.966\n",
      "Test set score: 0.761\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression \n",
    "clf = LogisticRegression() #분류기 선언\n",
    "clf.fit(X_train_tfidf, y_train) # train data를 이용하여 분류기를 학습\n",
    "print('Train set score: {:.3f}'.format(clf.score(X_train_tfidf, y_train))) # train data에 대한 예측정확도 \n",
    "print('Test set score: {:.3f}'.format(clf.score(X_test_tfidf, y_test))) # test data에 대한 예측정확도"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2034, 26550)\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(token_pattern= \"[a-zA-Z']{3,}\", decode_error ='ignore', lowercase=True, stop_words = stopwords.words('english'),ngram_range=(1, 2),max_df=0.5,min_df=2).fit(X_train)\n",
    "# Bigram 이기에 ngram_range 를 설정(1,2)\n",
    "X_train_tfidf = tfidf.transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)\n",
    "\n",
    "print(X_train_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'cause can't\", \"'em better\", \"'expected errors'\", \"'karla' next\", \"'nodis' password\", \"'official doctrine\", \"'ok see\", \"'sci astro'\", \"'what's moonbase\", 'aas american']\n"
     ]
    }
   ],
   "source": [
    "bigram_features = [f for f in tfidf.get_feature_names() if len(f.split()) > 1]\n",
    "# tfidf의 feature 명에서 두개 이상의 단어로 이루어진 것들\n",
    "print(bigram_features[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set score: 0.969\n",
      "Test set score: 0.756\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression() #분류기 선언\n",
    "clf.fit(X_train_tfidf, y_train) # train data를 이용하여 분류기를 학습\n",
    "print('Train set score: {:.3f}'.format(clf.score(X_train_tfidf, y_train))) # train data에 대한 예측정확도 \n",
    "print('Test set score: {:.3f}'.format(clf.score(X_test_tfidf, y_test))) # test data에 대한 예측정확도"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2034, 32943)\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(token_pattern= \"[a-zA-Z']{3,}\", decode_error ='ignore', lowercase=True, stop_words = stopwords.words('english'),ngram_range=(1, 3),max_df=0.5,min_df=2).fit(X_train)\n",
    "# Trigram 이기에 ngram_range 설정(1,3)\n",
    "X_train_tfidf = tfidf.transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)\n",
    "\n",
    "print(X_train_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'em better shots\", \"'expected errors' basically\", \"'karla' next one\", \"'nodis' password also\", \"'official doctrine think\", \"'ok see warning\", \"'what's moonbase good\", 'aas american astronautical', 'ability means infallible', 'able accept donations']\n"
     ]
    }
   ],
   "source": [
    "trigram_features = [f for f in tfidf.get_feature_names() if len(f.split()) > 2]\n",
    "# 단어 수가 2개 이상으로 이루어진 것들 추출\n",
    "print(trigram_features[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set score: 0.969\n",
      "Test set score: 0.758\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression() #분류기 선언\n",
    "clf.fit(X_train_tfidf, y_train) # train data를 이용하여 분류기를 학습\n",
    "print('Train set score: {:.3f}'.format(clf.score(X_train_tfidf, y_train))) # train data에 대한 예측정확도 \n",
    "print('Test set score: {:.3f}'.format(clf.score(X_test_tfidf, y_test))) # test data에 대한 예측정확도"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set score: 0.976\n",
      "Test set score: 0.775\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import RidgeClassifier\n",
    "ridge_clf = RidgeClassifier() #릿지 분류기 선언\n",
    "ridge_clf.fit(X_train_tfidf, y_train) #학습\n",
    "print('Train set score: {:.3f}'.format(ridge_clf.score(X_train_tfidf, y_train))) # train data에 대한 예측정확도\n",
    "print('Test set score: {:.3f}'.format(ridge_clf.score(X_test_tfidf, y_test))) # test data에 대한 예측정확도"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set score: 0.761\n",
      "Test set score: 0.695\n",
      "Used features count: 246 out of 32943\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "lasso_clf = LogisticRegression(penalty='l1', solver='liblinear') # Lasso는 동일한 LogisticRegression을 사용하면서 매개변수로 지정\n",
    "lasso_clf.fit(X_train_tfidf, y_train) # train data로 학습\n",
    "print('Train set score: {:.3f}'.format(lasso_clf.score(X_train_tfidf, y_train)))\n",
    "print('Test set score: {:.3f}'.format(lasso_clf.score(X_test_tfidf, y_test)))\n",
    "print('Used features count: {}'.format(np.sum(lasso_clf.coef_ != 0)), 'out of', X_train_tfidf.shape[1]) # 유용한 feature 개수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "clf = SVC(gamma='auto', kernel='linear')\n",
    "clf.fit(X_train_tfidf, y_train) \n",
    "print('Train set score: {:.3f}'.format(clf.score(X_train_tfidf, y_train))) # train data에 대한 예측정확도 \n",
    "print('Test set score: {:.3f}'.format(clf.score(X_test_tfidf, y_test))) # test data에 대한 예측정확도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
