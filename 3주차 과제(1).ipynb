{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 웹 크롤링1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 실습코드\n",
    "Q: 실습 파트의 코드를 작성하고 그 코드를 작성한 이유에 대해서 (강의에서 배운 내용 위주로) 주석으로 작성해주기 바랍니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. urllib\n",
    "* 파이썬은 웹 사이트에 있는 데이터를 추출하기 위해 urllib 라이브러리 사용\n",
    "* 이를 이용해 HTTP 또는 FTP를 사용해 데이터 다운로드 가능\n",
    "* urllib 은 URL을 다루는 모듈을 모아 놓은 패키지\n",
    "* urllib.request 모듈은 웹 사이트에 있는 데이터에 접근하는 기능 제공, 또한 인증, 리다렉트, 쿠키처럼 인터넷을 이용한 다양한 요청과 처리가 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib import request    # urllib 라이브러리 불러오기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 urllib.request 를 이용한 다운로드\n",
    "* urllib.re1quest 모듈에 있는 urlretrieve()함수 이용\n",
    "* 다음의 코드는 PNG 파일을 test.png 라는 이름의 파일로 저장하는 예제임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "저장되었습니다\n"
     ]
    }
   ],
   "source": [
    "# 라이브러리 읽어들이기\n",
    "from urllib import request\n",
    "\n",
    "url = \"http://uta.pw/shodou/img/28/214.png\"     # url 변수에 주소 입력\n",
    "savename = \"test.png\"           # 파일 저장\n",
    "\n",
    "request.urlretrieve(url,savename)    # 파일에 직접 저장\n",
    "print(\"저장되었습니다\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 urlopen으로 파일에 저장하는 방법\n",
    "* request.urlopen()은 메모리에 데이터를 올린 후 파일에 저장하게 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "저장되었습니다..\n"
     ]
    }
   ],
   "source": [
    "# URL과 저장경로 지정하기\n",
    "url = \"http://uta.pw/shodou/img/28/214.png\"\n",
    "savename = \"test1.png\"\n",
    "#다운로드\n",
    "mem = request.urlopen(url).read()\n",
    "#파일로 저장하기, wb는 쓰기와 바이너리모드\n",
    "with open(savename, mode=\"wb\") as f:   \n",
    "    f.write(mem)               # f.write 를 씀으로써 화면에 출력이 아닌 파일에 결과값을 반영\n",
    "    print(\"저장되었습니다..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 API 사용하기\n",
    "### 클라이언트 접속 정보 출력 (기본)\n",
    "* API는 사용자의 요청에 따라 정보를 반환하는 프로그램\n",
    "* IP주소, UserAgent 등 클라이언트 접속정보 출력하는 \"IP확인 API\" 접근해서 정보를 추출하는 프로그램"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ip]\n",
      "API_URI=http://api.aoikujira.com/ip/get.php\n",
      "REMOTE_ADDR=221.153.206.48\n",
      "REMOTE_HOST=221.153.206.48\n",
      "REMOTE_PORT=38926\n",
      "HTTP_HOST=api.aoikujira.com\n",
      "HTTP_USER_AGENT=Python-urllib/3.8\n",
      "HTTP_ACCEPT_LANGUAGE=\n",
      "HTTP_ACCEPT_CHARSET=\n",
      "SERVER_PORT=80\n",
      "FORMAT=ini\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 데이터 읽어들이기\n",
    "url = \"http://api.aoikujira.com/ip/ini\"\n",
    "res = request.urlopen(url)\n",
    "data = res.read()  # .read 를 이용해서 주소의 내용을 data에 저장\n",
    "\n",
    "# 바이너리를 문자열로 변환하기\n",
    "text = data.decode(\"utf-8\")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. BeautifulSoup\n",
    "* 스크레이핑(Scraping or Crawling)이란 웹 사이트에서 데이터를 추출하고, 원하는 정보를 추출하는 것을 의미\n",
    "* BeautifulSoup 란 파이썬으로 스크레이핑할 때 사용되는 라이브러리로서 HTML/XML 에서 정보를 추출할 수 있도록 도와줌. 그러나 다운로드 기능은 없음\n",
    "* 파이썬 라이브러리는 pip 명령어를 이용해 설치 가능. Python Package Index(PyPI)에 있는 패키지 명령어를 한줄로 설치 가능\n",
    "pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 패키지 import 및 예제 HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = \"\"\"             \n",
    "<html><body>\n",
    "    <h1>스크레이핑이란?</h1>\n",
    "    <p>웹 페이지를 분석하는 것</p>\n",
    "    <p>원하는 부분을 추출하는 것</p>\n",
    "</body><html>\n",
    "\"\"\"      # html에 태그 별 내용 저장"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 기본 사용\n",
    "* 다음은 Beautifulsoup를 이용하여 웹사이트로부터 HTML을 가져와 문자열로 만들어 이용하는 예제임\n",
    "* h1 태그를 접근하기 위해 html-body-h1 구조를 사용하여 soup.html.body.h1 이런식으로 이용하게 됨.\n",
    "* p 태그는 두개가 있어 soup.html.body.p 한 후 next_sibling을 두 번 이용하여 다음 p를 추출. 한번만 하면 그 다음 공백이 추출됨.\n",
    "* HTML 태그가 복잡한 경우 이런 방식으로 계속 진행하기는 적합하지 않음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) HTML 분석하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html, 'html.parser')  #BeautifulSoup를 이용하여 html를 불러옴"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) 원하는 부분 출력하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "h1 = soup.html.body.h1         # h1태그 저장\n",
    "p1 = soup.html.body.p          # p의 첫번째 문장 저장\n",
    "p2 = p1.next_sibling.next_sibling  # p의 다음 문장 저장"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) 요소의 글자 출력하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h1 = 스크레이핑이란?\n",
      "p = 웹 페이지를 분석하는 것\n",
      "p = 원하는 부분을 추출하는 것\n"
     ]
    }
   ],
   "source": [
    "print(f\"h1 = {h1.string}\")    \n",
    "print(f\"p = {p1.string}\")\n",
    "print(f\"p = {p2.string}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 요소를 찾는 method\n",
    "### 단일 element 추출: find()\n",
    "BeautifulSoup는 루트부터 하나하나 요소를 찾는 방법 말고도 find()라는 메소드를 제공함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 1) find() 메서드로 원하는 부분 추출하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h1>스크레이핑이란?</h1>\n"
     ]
    }
   ],
   "source": [
    "title = soup.find(\"h1\")      # h1태그 부분을 찾아 저장\n",
    "body = soup.find(\"p\")\n",
    "print(title)                # title 변수 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 2) 텍스트 부분 출력하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#title = 스크레이핑이란?\n",
      "#body = 웹 페이지를 분석하는 것\n"
     ]
    }
   ],
   "source": [
    "print(f\"#title = {title.string}\" )         # title 부분 텍스트 출력\n",
    "print(f\"#body = {body.string}\" )           # body 부분 텍스트 출력    태그 p의 첫문장밖에 출력되지 않음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 복수 elements 추출: find_all()\n",
    "여러개의 태그를 한번에 추출하고자 할때 사용함. 다음의 예제에서는 여러개의 태그를 추출하는 법을 보여주고 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = \"\"\" \n",
    "<html><body>\n",
    "    <ul>\n",
    "        <li><a href=\"http://www.naver.com\">naver</a></li>\n",
    "        <li><a href=\"http://www/daum.net\">daum</a></li>\n",
    "    </ul>\n",
    "</body></html>\n",
    "\"\"\"                    # html 의 태그 요소들 저장\n",
    "\n",
    "soup = BeautifulSoup(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 1) find_all() 메서드로 추출하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<a href=\"http://www.naver.com\">naver</a>, <a href=\"http://www/daum.net\">daum</a>] 2\n"
     ]
    }
   ],
   "source": [
    "links = soup.find_all(\"a\")       # a href로 되어있는 태그 부분 링크에 저장\n",
    "print(links, len(links))         # links에 들어가있는 태그 다 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 2) 링크 목록 출력하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naver > http://www.naver.com\n",
      "daum > http://www/daum.net\n"
     ]
    }
   ],
   "source": [
    "for a in links:\n",
    "    href = a.attrs['href'] # href의 속성에 있는 속성값을 추출\n",
    "    text = a.string     # 링크에 있는 문구 text 변수에 저장\n",
    "    print(text, \">\", href)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Css Selector\n",
    "Css Selector란, 웹상의 요소에 css를 적용하기 위한 문법으로, 즉 요소를 선택하기 위한 패턴입니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BeautifulSoup 에서 Css Selector 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = \"\"\"\n",
    "<html><body>\n",
    "<div id =  \"meigen\">\n",
    "    <h1>위키북스 도서</h1>\n",
    "    <ul class = \"items\">\n",
    "        <li>유니티 게임 이펙트 입문</li>\n",
    "        <li>스위프트로 시작하는 아이폰 앱 개발 교과서</li>\n",
    "        <li>모던 웹사이트 디자인의 정석</li>\n",
    "    </ul>\n",
    "</div>\n",
    "</body></html>\n",
    "\"\"\"\n",
    "\n",
    "# HTML 분석하기\n",
    "soup = BeautifulSoup(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 필요한 부분을 CSS 쿼리로 추출하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h1 = 위키북스 도서\n",
      "li = 유니티 게임 이펙트 입문\n",
      "li = 스위프트로 시작하는 아이폰 앱 개발 교과서\n",
      "li = 모던 웹사이트 디자인의 정석\n"
     ]
    }
   ],
   "source": [
    "# 타이틀 부분 추출하기 --- (#3)\n",
    "h1 = soup.select_one(\"div#meigen > h1\").string   # div meigen의 h1 태그 내용 문자열로 저장\n",
    "print(f\"h1 = {h1}\")\n",
    "\n",
    "# 목록 부분 추출하기 --- (#4)\n",
    "li_list = soup.select(\"div#meigen > ul.items > li\")  # div meigen의 ul items 에서 li태그 내용 저장\n",
    "for li in li_list:    \n",
    "    print(f\"li = {li.string}\")   # 순서대로 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 활용 예제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞서 배운 urllib과 BeautifulSoup를 조합하면, 웹스크레이핑 및 API요청 작업을 쉽게 수행하실 수 있습니다.\n",
    "\n",
    "1. URL을 이용하여 웹으로부터 html을 읽어들임(urllib)\n",
    "2. html 분석 및 원하는 데이터를 추출(BeautifulSoup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib import request, parse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. 네이버 금융 - 환율 정보\n",
    "* 다양한 금융 정보가 공개돼 있는 \"네이버 금융\"에서 원/달러 환율 정보를 추출해보자!\n",
    "* 네이버 금융의 시장 지표 페이지 http://finance.naver.com/marketindex/\n",
    "* 다음은 원/달러 환율 정보를 추출하는 프로그램임"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) HTML 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://finance.naver.com/marketindex/\"       \n",
    "res = request.urlopen(url)     # 주소 res 변수에 저장."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) HTML 분석하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(res, \"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) 원하는 데이터 추출하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usd/krw = 1,173.00\n"
     ]
    }
   ],
   "source": [
    "price = soup.select_one(\"div.head_info > span.value\").string   # span.value 태그 부분중 첫번째 요소만 반영하여 문자열로 저장.\n",
    "print(\"usd/krw =\", price) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. 기상청 RSS\n",
    "* 기상청 RSS에서 특정 내용을 추출하는 예제\n",
    "* 기상청 RSS에서 XML 데이터를 추출하고 XML 내용을 출력\n",
    "* 기상청의 RSS 서비스에 지역 번호를 지정하여 데이터 요청해보기\n",
    "http://www.kma.go.kr/weather/forecast/mid-term-rss3.jsp\n",
    "* 참고: 기상청 RSS: http://www.kma.go.kr/weather/lifenindustry/service_rss.jsp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 파이썬으로 요청 전용 매개변수를 만들 때는 urllib.parse 모듈의 urlencode() 함수를 사용해 매개변수를 URL로 인코딩한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) HTML 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "url= http://www.kma.go.kr/weather/forecast/mid-term-rss3.jsp?stnld=109\n"
     ]
    }
   ],
   "source": [
    "url = \"http://www.kma.go.kr/weather/forecast/mid-term-rss3.jsp\"\n",
    "\n",
    "#매개변수를 URL로 인코딩한다.\n",
    "values = {\n",
    "    'stnld':'109'               # css 선택자 속성\n",
    "}\n",
    "\n",
    "params=parse.urlencode(values)        # URL을 튜플의 하위 클래스인 ParseResult 클래스로 변환할 수 있다.\n",
    "url += \"?\"+params # URL에 매개변수 추가\n",
    "print(\"url=\",url)\n",
    "\n",
    "res = request.urlopen(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) HTML 분석하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(res, \"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) 원하는 데이터 추출하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전국 육상중기예보\n",
      "○ (기온) 이번 예보기간 낮 기온은 20~25도로 어제(21~28도)와 비슷하거나 조금 낮겠고, 아침 기온은 10~18도로 선선하겠습니다. <br />          특히, 내륙을 중심으로 낮과 밤의 기온차가 10도 내외로 크겠습니다.<br />○ (해상) 28일(월)은 동해상과 남해상에서 물결이 2.0~4.0m로 매우 높게 일겠습니다.\n"
     ]
    }
   ],
   "source": [
    "header = soup.find(\"header\")\n",
    "\n",
    "title = header.find(\"title\").text\n",
    "wf = header.find(\"wf\").text\n",
    "\n",
    "print(title)\n",
    "print(wf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* css selector 기반"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-101-d8c15b9416d1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtitle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect_one\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"header > title\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mwf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect_one\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"header wf\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "title = soup.select_one(\"header > title\").text\n",
    "wf = header.select_one(\"header wf\").text\n",
    "\n",
    "print(title)\n",
    "print(wf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3. 윤동주 작가의 작품 목록\n",
    "* 위키문헌에 (https://ko.wikisource.org/wiki) 공개되어 있는 윤동주의 작품목록을 가져오기\n",
    "* 윤동주 위키\n",
    "  - (https://ko.wikisource.org/wiki/%EC%A0%80%EC%9E%90:%EC%9C%A4%EB%8F%99%EC%A3%BC)\n",
    "* 하늘과 바람과 시 부분을 선택한 후 오른쪽 마우스 이용해 copy selector로 카피하면 다음의 CSS선택자가 카피됨\n",
    "   - #mw-content-text > div > ul:nth-child(6) > li > b > a\n",
    "* nth-child(n) 은 n 번째 요소를 의미, 즉 6번째 요소를 의미, #mw-content-text 내부에 있는 url 태그는 모두 작품과 관련된 태그. 따라서 따로 구분할 필요는 없으며 생략해도 됨. BeautifulSoup nth-child 지원하지 않음\n",
    "   - Recall PR7 Problem1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://ko.wikisource.org/wiki/%EC%A0%80%EC%9E%90:%EC%9C%A4%EB%8F%99%EC%A3%BC\"\n",
    "res = request.urlopen(url)\n",
    "soup = BeautifulSoup(res, \"html.parser\")\n",
    "\n",
    "# #mw-content-text 바로 아래에 있는\n",
    "# ul 태그 바로 아래에 있는\n",
    "# li 태그 아래에 있는\n",
    "# a 태그를 모두 선택합니다  -> select_one과 다른점\n",
    "a_list = soup.select(\"#mw-content-text   ul > li a\")\n",
    "for a in a_list:\n",
    "    name = a.string\n",
    "    print(f\"- {name}\", )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 일반문제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib import request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 네이버 뉴스 헤드라인\n",
    "배운 내용을 바탕으로 네이버 뉴스(https://news.naver.com/)에서 헤드라인 뉴스의 제목을 추출해보고자 합니다.\n",
    "Q: 다음의 코드에 css selector를 추가하여 최신 기사의 헤드라인을 스크레이핑하는 코드를 완성하시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://news.naver.com/\"\n",
    "\n",
    "res = request.urlopen(url)\n",
    "soup = BeautifulSoup(res, \"html.parser\")\n",
    "\n",
    "selector = \"#today_main_news > div.hdline_news > ul > li > div.hdline_article_title > a\"\n",
    "\n",
    "for a in soup.select(selector):\n",
    "    title = a.text\n",
    "    print(title)  \n",
    "    \n",
    "# 출력이 안되는데 이유는 아마 네이버에서 링크를 타고 오는 것을 막아서 그런 것 같다.\n",
    "# 링크를 타고 눌러보면 뉴스 페이지가 뜨지 않고 오류가 뜬다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 시민의 소리 게시판\n",
    "다음은 서울시 대공원의 시민의 소리 게시판입니다.\n",
    "\n",
    "https://www.sisul.or.kr/open_content/childrenpart/qna/qnaMsgList.do?pgno=1\n",
    "해당 페이지에 나타난 게시글들의 제목을 수집하고자 합니다.\n",
    "Q: 다음의 코드에 css selector를 추가하여 해당 페이지에서 게시글의 제목을 스크레이핑하는 코드를 완성하시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_head = \"https://www.sisul.or.kr\"\n",
    "\n",
    "url_board = url_head + \"/open_content/childrenpark/qna/qnaMsgList.do?pgno=1\"\n",
    "# url 주소와 특정 페이지 주소 합침\n",
    "\n",
    "res = request.urlopen(url_board)    # 특정페이지 주소 반환\n",
    "soup = BeautifulSoup(res, \"html.parser\")     # python 객체로 변환\n",
    "\n",
    "#selector = \"#detail.con > div.generalboard > table > tbody > tr > td.left.title > a\"\n",
    "selector = \"#detail_con > div.generalboard > table > tbody > tr > td.left.title > a\"  # title들 selector 변수에 저장\n",
    "titles = []\n",
    "links = []\n",
    "for a in soup.select(selector):\n",
    "    titles.append(a.text)     # 선택자에서 타이틀 리스트에 추가\n",
    "    links.append(url_head + a.attrs[\"href\"])  # url_head와 href의 인자를 넘겨 합친다.\n",
    "    \n",
    "print(titles, links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 추가 내용\n",
    "수집된 자료를 데이터프레임으로 만들어 csv로 저장하는 것이 일반적입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "board_df = pd.DataFrame({\"title\":titles, \"link\": links})     # dataframe을 생성한다. titles과 links로 구성\n",
    "board_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
