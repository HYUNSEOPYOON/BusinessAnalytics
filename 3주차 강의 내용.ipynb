{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.Tokenizing text into sentences\n",
    "Tokenization이란 문자열을 여러개의 조각으로 나누는 것.\n",
    "Token은 문자열의 한 조각으로 하나의 단어가 하나의 토큰이라고 할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) paragraph를 문장 단위로 tokenize 하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['My name is YOONHYUNSEOP.', 'My major is e-Business.', 'Thanks you.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize  # 문장 단위로 tokenize\n",
    "para = \"My name is YOONHYUNSEOP. My major is e-Business. Thanks you.\"\n",
    "print(sent_tokenize(para))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Tokenizing sentences into words\n",
    "가장 기본적인 word_tokenize 함수. Space 단위와 구두점(punctuation)을 기준으로 토큰화한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['My', 'name', 'is', 'YOONHYUNSEOP', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "print(word_tokenize(\"My name is YOONHYUNSEOP.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Separating contractions\n",
    "word_tokenize()를 사용한 can`t의 분리 예시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ca', \"n't\"]\n",
      "['can', '`', 't']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "print(word_tokenize(\"can't\")) # 신기한건 ` 와 ' 를 넣어서 했을 때 다르다는 점이다.\n",
    "print(word_tokenize(\"can`t\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WordPunctTokenizer\n",
    "\n",
    "word_tokenize( )의 대안인 WordPunctTokenizer.\n",
    "이 Tokenizer는 모든 구두점(punctuation)을 기준으로 분리한다고 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Can', \"'\", 't', 'is', 'a', 'contraction', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "tokenizer = WordPunctTokenizer()\n",
    "print(tokenizer.tokenize(\"Can't is a contraction.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 표제어 추출(Lemmatization)\n",
    "사전을 이용하여 단어의 원형을 추출\n",
    "* 품사(part-of-speech)를 고려\n",
    "* 영어의 경우, 유명한 어휘 데이터베이스인 WordNet을 이용한 WordNet lemmatizer가 많이 쓰임 (강의노트 및 수업 내용)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WordNetLemmatizer를 통한 표제어 추출 시도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['talking', 'doing', 'purpose', 'have', 'going', 'love', 'life', 'fly', 'dy', 'watched', 'ha', 'starting']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "n = WordNetLemmatizer()\n",
    "words = ['talking', 'doing', 'purpose', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
    "print([n.lemmatize(w) for w in words])\n",
    "# 품사를 모르기에 ha, dy와 같은 이상한 단어가 추출됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 단어의 품사를 알려줘서 표제어 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'die'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n.lemmatize('dies','v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'watch'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n.lemmatize('watched','v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'have'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n.lemmatize('has','v')\n",
    "# v를 적어 동사라고 알려주자 동사의 원형을 다 잘 추출해냄"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 어간 추출(Stemming)\n",
    "* Stemming (어간 추출)\n",
    "* 단수 – 복수, 현재형 – 미래형 등 단어의 다양한 변형을 하나로 통일\n",
    "* 의미가 아닌 규칙에 의한 변환\n",
    "* 영어의 경우, Porter stemmer, Lancaster stemmer 등이 유명"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Stemming 시도(포터 알고리즘)\n",
    "\n",
    "input : My name is HYUNSEOP and my major is e-Business. My grade is fourth and I want to get a job. After one years, I will graduate school. Now I actually do my last semester. I have remained 2 semester but there is no class in my last semester."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['My', 'name', 'is', 'HYUNSEOP', 'and', 'my', 'major', 'is', 'e-Business', '.', 'My', 'grade', 'is', 'fourth', 'and', 'I', 'want', 'to', 'get', 'a', 'job', '.', 'After', 'one', 'years', ',', 'I', 'will', 'graduate', 'school', '.', 'Now', 'I', 'actually', 'do', 'my', 'last', 'semester', '.', 'I', 'have', 'remained', '2', 'semester', 'but', 'there', 'is', 'no', 'class', 'in', 'my', 'last', 'semester', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer  # 포터 알고리즘(어간 추출 알고리즘 중 하나인) 불러옴\n",
    "from nltk.tokenize import word_tokenize\n",
    "s = PorterStemmer()\n",
    "text = \"My name is HYUNSEOP and my major is e-Business. My grade is fourth and I want to get a job. After one years, I will graduate school. Now I actually do my last semester. I have remained 2 semester but there is no class in my last semester.\"\n",
    "words = word_tokenize(text) # 단어로 토큰화하여 넣음.\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['My', 'name', 'is', 'hyunseop', 'and', 'my', 'major', 'is', 'e-busi', '.', 'My', 'grade', 'is', 'fourth', 'and', 'I', 'want', 'to', 'get', 'a', 'job', '.', 'after', 'one', 'year', ',', 'I', 'will', 'graduat', 'school', '.', 'now', 'I', 'actual', 'do', 'my', 'last', 'semest', '.', 'I', 'have', 'remain', '2', 'semest', 'but', 'there', 'is', 'no', 'class', 'in', 'my', 'last', 'semest', '.']\n"
     ]
    }
   ],
   "source": [
    "print([s.stem(w) for w in words]) # 알고리즘에 의해 추출하였더니 이상하게 나오는 단어도 발생한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Stemming 시도(랭커스터 알고리즘)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['my', 'nam', 'is', 'hyunseop', 'and', 'my', 'maj', 'is', 'e-business', '.', 'my', 'grad', 'is', 'four', 'and', 'i', 'want', 'to', 'get', 'a', 'job', '.', 'aft', 'on', 'year', ',', 'i', 'wil', 'gradu', 'school', '.', 'now', 'i', 'act', 'do', 'my', 'last', 'semest', '.', 'i', 'hav', 'remain', '2', 'semest', 'but', 'ther', 'is', 'no', 'class', 'in', 'my', 'last', 'semest', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "l = LancasterStemmer()\n",
    "print([l.stem(w) for w in words])  # 알고리즘이 다르기에 전혀 다른 단어들 추출\n",
    "\n",
    "# 예를들어 포터알고리즘은 name 이 제데로 되고 e-business가 깨지지만, 랭캐스터는 name이 깨지고 e-business가 제대로 출력된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part-Of-Speech tagging(pos tagging)\n",
    "* 토큰화와 정규화 작업을 통해 나누어진 형태소(의미를 가지는 최소단위)에 대해 품사를 결정하여 할당하는 작업\n",
    "* 동일한 단어라도 문맥에 따라 의미가 달라지므로 품사를 알기 위해서는 문맥을 파악해야 함\n",
    "* POS Tagging은 형태소 분석으로 번역되기도 하는데, 형태소 분석은 주어진 텍스트(원시말뭉치)를 형태소 단위로 나누는 작업을 포함하므로 앞의 토큰화, 정규화 작업에 품사 태깅을 포함한 것으로 보는 것이 타당\n",
    "* 문장 내 단어들의 품사를 식별하여 태그를 붙여주는 것. 투플(tuple)의 형태로 출력되며 (단어, 태그)로 출력. 여기서 태그는 품사(POS) 태그.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Default tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Hello', 'NN'), ('World', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tag.sequential import DefaultTagger   # 태그 모듈 \n",
    "tagger = DefaultTagger('NN')\n",
    "print(tagger.tag(['Hello','World']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Evaluating accuracy\n",
    "tagger의 정확도를 측정하려면 evaluate() 메소드를 사용하여 확인할 수 있다고 한다.\n",
    "아래의 예제는 treebank corpus의 tagged_sents 일부와 비교 시도."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14331966328512843\n"
     ]
    }
   ],
   "source": [
    "from nltk.tag.sequential import DefaultTagger\n",
    "tagger = DefaultTagger('NN')\n",
    "from nltk.corpus import treebank\n",
    "test_sents = treebank.tagged_sents()[3000:]\n",
    "print(tagger.evaluate(test_sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Untagging a tagged sentence\n",
    "\n",
    "Tagging된 토큰들을 nltk.tag.untag( )를 이용하여 태그들을 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'World']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tag.util import untag\n",
    "print(untag([('Hello','NN'),('World','NN')]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Training a unigram part-of-speech tagger\n",
    "\n",
    "unigram은 일반적으로 단일 토큰을 참조하며, unigram tagger는 POS tag를 결정하기위한 컨텍스트로 한 단어만 사용한다. UnigramTagger는 SequentialBackoffTagger에서 상속받은 ContextTagger의 하위 클래스인 NgramTagger로 부터 상속받는다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Pierre', 'Vinken', ',', '61', 'years', 'old', ',', 'will', 'join', 'the', 'board', 'as', 'a', 'nonexecutive', 'director', 'Nov.', '29', '.']\n",
      "[('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ('61', 'CD'), ('years', 'NNS'), ('old', 'JJ'), (',', ','), ('will', 'MD'), ('join', 'VB'), ('the', 'DT'), ('board', 'NN'), ('as', 'IN'), ('a', 'DT'), ('nonexecutive', 'JJ'), ('director', 'NN'), ('Nov.', 'NNP'), ('29', 'CD'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import treebank\n",
    "from nltk.tag.sequential import UnigramTagger\n",
    "\n",
    "train_sents = treebank.tagged_sents()[:3000]  # treebank.tagged_sents의 단어들을 넣는다.\n",
    "tagger = UnigramTagger(train_sents)   # Unigram을 통해 한단어씩만 사용.\n",
    "print(treebank.sents()[0])\n",
    "print(tagger.tag(treebank.sents()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ('61', 'CD'), ('years', 'NNS'), ('old', 'JJ'), (',', ','), ('will', 'MD'), ('join', 'VB'), ('the', 'DT'), ('board', 'NN'), ('as', 'IN'), ('a', 'DT'), ('nonexecutive', 'JJ'), ('director', 'NN'), ('Nov.', 'NNP'), ('29', 'CD'), ('.', '.')], [('Mr.', 'NNP'), ('Vinken', 'NNP'), ('is', 'VBZ'), ('chairman', 'NN'), ('of', 'IN'), ('Elsevier', 'NNP'), ('N.V.', 'NNP'), (',', ','), ('the', 'DT'), ('Dutch', 'NNP'), ('publishing', 'VBG'), ('group', 'NN'), ('.', '.')], ...]\n"
     ]
    }
   ],
   "source": [
    "print(treebank.tagged_sents())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 개체명 인식(NER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK를 이용한 개체명 인식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Hyunseop', 'NNP'), ('is', 'VBZ'), ('studying', 'VBG'), ('about', 'IN'), ('Python', 'NNP'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "sentence = \"Hyunseop is studying about Python.\"\n",
    "sentence = pos_tag(word_tokenize(sentence))  # 문장을 단어로 토큰화\n",
    "print(sentence)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (GPE Hyunseop/NNP)\n",
      "  is/VBZ\n",
      "  studying/VBG\n",
      "  about/IN\n",
      "  (PERSON Python/NNP)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "sentence = ne_chunk(sentence)\n",
    "print(sentence)    # 개체명 인식 # Hyunseop이 person으로 안뜨는걸로 보아 사람으로 인식을 못한 것 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bow(Bag of Words)\n",
    "\n",
    "* 특징: 단어들의 순서를 전혀 고려하지 않고 출현 빈도(frequency)에만 집중.\n",
    "* 문장을 넣어 연습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'나': 0, '는': 1, '현재': 2, '오': 3, '훈': 4, '이': 5, '와': 6, '바빈스': 7, '커피숍': 8, '에서': 9, '공부': 10, '를': 11, '하고': 12, '있고': 13, '도': 14, '있다': 15}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 2, 2, 2, 1, 1, 1, 1, 2, 2, 2, 1, 1, 1]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "import re\n",
    "okt = Okt()\n",
    "token = re.sub(\"(\\.)\",\"\",\"나는 현재 오훈이와 바빈스 커피숍에서 공부를 하고 있고 오훈이도 공부를 하고 있다.\")\n",
    "# 정규 표현식(re.sub)을 통해 온점을 제거.\n",
    "\n",
    "token = okt.morphs(token)\n",
    "# Okt 형태소 분석기를 통해 토큰화 작업을 수행한 뒤에, token에다가 넣는다.\n",
    "\n",
    "word2index = {}\n",
    "bow = []\n",
    "for voca in token:\n",
    "    if voca not in word2index.keys():\n",
    "        word2index[voca] = len(word2index) # 토큰을 읽으면서 word2index에 없는 (not in) 단어는 새로 추가하고, 이미 있는 단어는 넘긴다.\n",
    "        bow.insert(len(word2index)-1,1) # bow 전체에 기본값 1 설정, 단어의 개수는 최소 1개 이상이기 때문\n",
    "    \n",
    "    else:\n",
    "        index = word2index.get(voca) # 재등장하는 단어의 인덱스를 받아옵니다.\n",
    "        bow[index] = bow[index] + 1 # 재등장하는 단어는 해당하는 인덱스의 위치에 1을 더해줍니다. (단어의 개수를 세는 것)\n",
    "        \n",
    "print(word2index)\n",
    "bow # 인덱스의 위치별로 나온 횟수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from math import log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = ['먹고 싶은 사과', '먹고 싶은 바나나', '길고 노란 바나나 바나나', '저는 과일이 좋아요']\n",
    "vocab = list(set(w for doc in docs for w in doc.split())) # 단어들을 중복을 허용하지 않고 list로 만든다\n",
    "vocab.sort() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['과일이', '길고', '노란', '먹고', '바나나', '사과', '싶은', '저는', '좋아요']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* TF,IDF, 그리고 TF-IDF 값을 구하는 함수 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(docs) # 총 문서의 수\n",
    "def tf(t, d):\n",
    "    return d.count(t)\n",
    "\n",
    "def idf(t):\n",
    "    df = 0\n",
    "    for doc in docs:\n",
    "        df += t in doc\n",
    "    return log(N/(df + 1))\n",
    "\n",
    "def tfidf(t, d):\n",
    "    return tf(t,d)* idf(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* TF 구하기, DTM을 데이터 프레임에 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>과일이</th>\n",
       "      <th>길고</th>\n",
       "      <th>노란</th>\n",
       "      <th>먹고</th>\n",
       "      <th>바나나</th>\n",
       "      <th>사과</th>\n",
       "      <th>싶은</th>\n",
       "      <th>저는</th>\n",
       "      <th>좋아요</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   과일이  길고  노란  먹고  바나나  사과  싶은  저는  좋아요\n",
       "0    0   0   0   1    0   1   1   0    0\n",
       "1    0   0   0   1    1   0   1   0    0\n",
       "2    0   1   1   0    2   0   0   0    0\n",
       "3    1   0   0   0    0   0   0   1    1"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = []\n",
    "for i in range(N):\n",
    "    result.append([])\n",
    "    d = docs[i]\n",
    "    for j in range(len(vocab)):\n",
    "        t = vocab[j]\n",
    "        result[-1].append(tf(t, d))\n",
    "\n",
    "tf_ = pd.DataFrame(result, columns = vocab)\n",
    "tf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IDF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>과일이</th>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>길고</th>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>노란</th>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>먹고</th>\n",
       "      <td>0.287682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>바나나</th>\n",
       "      <td>0.287682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>사과</th>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>싶은</th>\n",
       "      <td>0.287682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>저는</th>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>좋아요</th>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          IDF\n",
       "과일이  0.693147\n",
       "길고   0.693147\n",
       "노란   0.693147\n",
       "먹고   0.287682\n",
       "바나나  0.287682\n",
       "사과   0.693147\n",
       "싶은   0.287682\n",
       "저는   0.693147\n",
       "좋아요  0.693147"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = []\n",
    "for j in range(len(vocab)):\n",
    "    t = vocab[j]\n",
    "    result.append(idf(t))\n",
    "\n",
    "idf_ = pd.DataFrame(result, index = vocab, columns = [\"IDF\"])  # IDF 값\n",
    "idf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>과일이</th>\n",
       "      <th>길고</th>\n",
       "      <th>노란</th>\n",
       "      <th>먹고</th>\n",
       "      <th>바나나</th>\n",
       "      <th>사과</th>\n",
       "      <th>싶은</th>\n",
       "      <th>저는</th>\n",
       "      <th>좋아요</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287682</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.287682</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287682</td>\n",
       "      <td>0.287682</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287682</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.575364</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        과일이        길고        노란        먹고       바나나        사과        싶은  \\\n",
       "0  0.000000  0.000000  0.000000  0.287682  0.000000  0.693147  0.287682   \n",
       "1  0.000000  0.000000  0.000000  0.287682  0.287682  0.000000  0.287682   \n",
       "2  0.000000  0.693147  0.693147  0.000000  0.575364  0.000000  0.000000   \n",
       "3  0.693147  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "         저는       좋아요  \n",
       "0  0.000000  0.000000  \n",
       "1  0.000000  0.000000  \n",
       "2  0.000000  0.000000  \n",
       "3  0.693147  0.693147  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = []\n",
    "for i in range(N):\n",
    "    result.append([])\n",
    "    d = docs[i]\n",
    "    for j in range(len(vocab)):\n",
    "        t = vocab[j]\n",
    "\n",
    "        result[-1].append(tfidf(t,d))\n",
    "\n",
    "tfidf_ = pd.DataFrame(result, columns = vocab)\n",
    "tfidf_           \n",
    "# TF-IDF 행렬"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 인터넷 검색을 통해 연습해보기\n",
    "\n",
    "## 1. 케라스 임베딩 층(Keras Embedding layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = ['nice great best amazing', 'stop lies', 'pitiful nerd', 'excellent work', 'supreme quality', 'bad', 'highly respectable']\n",
    "y_train = [1, 0, 0, 1, 1, 0, 1]\n",
    "# 문장과 레이블 데이터. 긍정인 문장은 레이블1, 부정인 문장은 레이블0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "t = Tokenizer()\n",
    "t.fit_on_texts(sentences)\n",
    "vocab_size = len(t.word_index) + 1\n",
    "\n",
    "print(vocab_size) \n",
    "# Tokenizer를 사용하여 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2, 3, 4], [5, 6], [7, 8], [9, 10], [11, 12], [13], [14, 15]]\n"
     ]
    }
   ],
   "source": [
    "X_encoded = t.texts_to_sequences(sentences)\n",
    "print(X_encoded)\n",
    "# 각 문장에 대해 정수 인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "max_len = max(len(l) for l in X_encoded)\n",
    "print(max_len)\n",
    "\n",
    "# 문장 중에서 가장 길이가 긴 문장의 길이는 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  2  3  4]\n",
      " [ 5  6  0  0]\n",
      " [ 7  8  0  0]\n",
      " [ 9 10  0  0]\n",
      " [11 12  0  0]\n",
      " [13  0  0  0]\n",
      " [14 15  0  0]]\n"
     ]
    }
   ],
   "source": [
    "X_train = pad_sequences(X_encoded, maxlen = max_len, padding = 'post')\n",
    "y_train = np.array(y_train)\n",
    "print(X_train)\n",
    "# 패딩:  모든 샘플의 길이를 동일하게 맞추어야할 때 사용.\n",
    "# 패딩사용하여 길이 4로 다 통일시켜줌.\n",
    "# 여기까지 훈련 데이터에 대한 전처리 과정이라고 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 설계\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, Flatten\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 4, input_length = max_len))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "# 출력층에 1개의 뉴런에 활성화 함수로는 시그모이드 함수를 사용하여 이진 분류를 수행합니다.\n",
    "# 이 부분은 후에 공부가 더 필요해 보인다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1/1 - 0s - loss: 0.6910 - acc: 0.4286\n",
      "Epoch 2/100\n",
      "1/1 - 0s - loss: 0.6892 - acc: 0.4286\n",
      "Epoch 3/100\n",
      "1/1 - 0s - loss: 0.6873 - acc: 0.5714\n",
      "Epoch 4/100\n",
      "1/1 - 0s - loss: 0.6854 - acc: 0.5714\n",
      "Epoch 5/100\n",
      "1/1 - 0s - loss: 0.6836 - acc: 0.7143\n",
      "Epoch 6/100\n",
      "1/1 - 0s - loss: 0.6817 - acc: 0.7143\n",
      "Epoch 7/100\n",
      "1/1 - 0s - loss: 0.6798 - acc: 0.7143\n",
      "Epoch 8/100\n",
      "1/1 - 0s - loss: 0.6780 - acc: 0.7143\n",
      "Epoch 9/100\n",
      "1/1 - 0s - loss: 0.6761 - acc: 0.7143\n",
      "Epoch 10/100\n",
      "1/1 - 0s - loss: 0.6742 - acc: 0.7143\n",
      "Epoch 11/100\n",
      "1/1 - 0s - loss: 0.6723 - acc: 0.7143\n",
      "Epoch 12/100\n",
      "1/1 - 0s - loss: 0.6705 - acc: 0.7143\n",
      "Epoch 13/100\n",
      "1/1 - 0s - loss: 0.6686 - acc: 0.7143\n",
      "Epoch 14/100\n",
      "1/1 - 0s - loss: 0.6667 - acc: 0.7143\n",
      "Epoch 15/100\n",
      "1/1 - 0s - loss: 0.6648 - acc: 0.7143\n",
      "Epoch 16/100\n",
      "1/1 - 0s - loss: 0.6629 - acc: 0.8571\n",
      "Epoch 17/100\n",
      "1/1 - 0s - loss: 0.6610 - acc: 1.0000\n",
      "Epoch 18/100\n",
      "1/1 - 0s - loss: 0.6591 - acc: 1.0000\n",
      "Epoch 19/100\n",
      "1/1 - 0s - loss: 0.6572 - acc: 1.0000\n",
      "Epoch 20/100\n",
      "1/1 - 0s - loss: 0.6553 - acc: 1.0000\n",
      "Epoch 21/100\n",
      "1/1 - 0s - loss: 0.6534 - acc: 1.0000\n",
      "Epoch 22/100\n",
      "1/1 - 0s - loss: 0.6515 - acc: 1.0000\n",
      "Epoch 23/100\n",
      "1/1 - 0s - loss: 0.6495 - acc: 1.0000\n",
      "Epoch 24/100\n",
      "1/1 - 0s - loss: 0.6476 - acc: 1.0000\n",
      "Epoch 25/100\n",
      "1/1 - 0s - loss: 0.6457 - acc: 1.0000\n",
      "Epoch 26/100\n",
      "1/1 - 0s - loss: 0.6437 - acc: 1.0000\n",
      "Epoch 27/100\n",
      "1/1 - 0s - loss: 0.6418 - acc: 1.0000\n",
      "Epoch 28/100\n",
      "1/1 - 0s - loss: 0.6398 - acc: 1.0000\n",
      "Epoch 29/100\n",
      "1/1 - 0s - loss: 0.6378 - acc: 1.0000\n",
      "Epoch 30/100\n",
      "1/1 - 0s - loss: 0.6359 - acc: 1.0000\n",
      "Epoch 31/100\n",
      "1/1 - 0s - loss: 0.6339 - acc: 1.0000\n",
      "Epoch 32/100\n",
      "1/1 - 0s - loss: 0.6319 - acc: 1.0000\n",
      "Epoch 33/100\n",
      "1/1 - 0s - loss: 0.6299 - acc: 1.0000\n",
      "Epoch 34/100\n",
      "1/1 - 0s - loss: 0.6279 - acc: 1.0000\n",
      "Epoch 35/100\n",
      "1/1 - 0s - loss: 0.6259 - acc: 1.0000\n",
      "Epoch 36/100\n",
      "1/1 - 0s - loss: 0.6239 - acc: 1.0000\n",
      "Epoch 37/100\n",
      "1/1 - 0s - loss: 0.6219 - acc: 1.0000\n",
      "Epoch 38/100\n",
      "1/1 - 0s - loss: 0.6199 - acc: 1.0000\n",
      "Epoch 39/100\n",
      "1/1 - 0s - loss: 0.6179 - acc: 1.0000\n",
      "Epoch 40/100\n",
      "1/1 - 0s - loss: 0.6158 - acc: 1.0000\n",
      "Epoch 41/100\n",
      "1/1 - 0s - loss: 0.6138 - acc: 1.0000\n",
      "Epoch 42/100\n",
      "1/1 - 0s - loss: 0.6117 - acc: 1.0000\n",
      "Epoch 43/100\n",
      "1/1 - 0s - loss: 0.6097 - acc: 1.0000\n",
      "Epoch 44/100\n",
      "1/1 - 0s - loss: 0.6076 - acc: 1.0000\n",
      "Epoch 45/100\n",
      "1/1 - 0s - loss: 0.6056 - acc: 1.0000\n",
      "Epoch 46/100\n",
      "1/1 - 0s - loss: 0.6035 - acc: 1.0000\n",
      "Epoch 47/100\n",
      "1/1 - 0s - loss: 0.6014 - acc: 1.0000\n",
      "Epoch 48/100\n",
      "1/1 - 0s - loss: 0.5993 - acc: 1.0000\n",
      "Epoch 49/100\n",
      "1/1 - 0s - loss: 0.5972 - acc: 1.0000\n",
      "Epoch 50/100\n",
      "1/1 - 0s - loss: 0.5951 - acc: 1.0000\n",
      "Epoch 51/100\n",
      "1/1 - 0s - loss: 0.5930 - acc: 1.0000\n",
      "Epoch 52/100\n",
      "1/1 - 0s - loss: 0.5909 - acc: 1.0000\n",
      "Epoch 53/100\n",
      "1/1 - 0s - loss: 0.5888 - acc: 1.0000\n",
      "Epoch 54/100\n",
      "1/1 - 0s - loss: 0.5867 - acc: 1.0000\n",
      "Epoch 55/100\n",
      "1/1 - 0s - loss: 0.5845 - acc: 1.0000\n",
      "Epoch 56/100\n",
      "1/1 - 0s - loss: 0.5824 - acc: 1.0000\n",
      "Epoch 57/100\n",
      "1/1 - 0s - loss: 0.5803 - acc: 1.0000\n",
      "Epoch 58/100\n",
      "1/1 - 0s - loss: 0.5781 - acc: 1.0000\n",
      "Epoch 59/100\n",
      "1/1 - 0s - loss: 0.5760 - acc: 1.0000\n",
      "Epoch 60/100\n",
      "1/1 - 0s - loss: 0.5738 - acc: 1.0000\n",
      "Epoch 61/100\n",
      "1/1 - 0s - loss: 0.5716 - acc: 1.0000\n",
      "Epoch 62/100\n",
      "1/1 - 0s - loss: 0.5695 - acc: 1.0000\n",
      "Epoch 63/100\n",
      "1/1 - 0s - loss: 0.5673 - acc: 1.0000\n",
      "Epoch 64/100\n",
      "1/1 - 0s - loss: 0.5651 - acc: 1.0000\n",
      "Epoch 65/100\n",
      "1/1 - 0s - loss: 0.5629 - acc: 1.0000\n",
      "Epoch 66/100\n",
      "1/1 - 0s - loss: 0.5608 - acc: 1.0000\n",
      "Epoch 67/100\n",
      "1/1 - 0s - loss: 0.5586 - acc: 1.0000\n",
      "Epoch 68/100\n",
      "1/1 - 0s - loss: 0.5564 - acc: 1.0000\n",
      "Epoch 69/100\n",
      "1/1 - 0s - loss: 0.5542 - acc: 1.0000\n",
      "Epoch 70/100\n",
      "1/1 - 0s - loss: 0.5520 - acc: 1.0000\n",
      "Epoch 71/100\n",
      "1/1 - 0s - loss: 0.5498 - acc: 1.0000\n",
      "Epoch 72/100\n",
      "1/1 - 0s - loss: 0.5476 - acc: 1.0000\n",
      "Epoch 73/100\n",
      "1/1 - 0s - loss: 0.5453 - acc: 1.0000\n",
      "Epoch 74/100\n",
      "1/1 - 0s - loss: 0.5431 - acc: 1.0000\n",
      "Epoch 75/100\n",
      "1/1 - 0s - loss: 0.5409 - acc: 1.0000\n",
      "Epoch 76/100\n",
      "1/1 - 0s - loss: 0.5387 - acc: 1.0000\n",
      "Epoch 77/100\n",
      "1/1 - 0s - loss: 0.5365 - acc: 1.0000\n",
      "Epoch 78/100\n",
      "1/1 - 0s - loss: 0.5342 - acc: 1.0000\n",
      "Epoch 79/100\n",
      "1/1 - 0s - loss: 0.5320 - acc: 1.0000\n",
      "Epoch 80/100\n",
      "1/1 - 0s - loss: 0.5298 - acc: 1.0000\n",
      "Epoch 81/100\n",
      "1/1 - 0s - loss: 0.5275 - acc: 1.0000\n",
      "Epoch 82/100\n",
      "1/1 - 0s - loss: 0.5253 - acc: 1.0000\n",
      "Epoch 83/100\n",
      "1/1 - 0s - loss: 0.5231 - acc: 1.0000\n",
      "Epoch 84/100\n",
      "1/1 - 0s - loss: 0.5208 - acc: 1.0000\n",
      "Epoch 85/100\n",
      "1/1 - 0s - loss: 0.5186 - acc: 1.0000\n",
      "Epoch 86/100\n",
      "1/1 - 0s - loss: 0.5163 - acc: 1.0000\n",
      "Epoch 87/100\n",
      "1/1 - 0s - loss: 0.5141 - acc: 1.0000\n",
      "Epoch 88/100\n",
      "1/1 - 0s - loss: 0.5118 - acc: 1.0000\n",
      "Epoch 89/100\n",
      "1/1 - 0s - loss: 0.5096 - acc: 1.0000\n",
      "Epoch 90/100\n",
      "1/1 - 0s - loss: 0.5073 - acc: 1.0000\n",
      "Epoch 91/100\n",
      "1/1 - 0s - loss: 0.5051 - acc: 1.0000\n",
      "Epoch 92/100\n",
      "1/1 - 0s - loss: 0.5028 - acc: 1.0000\n",
      "Epoch 93/100\n",
      "1/1 - 0s - loss: 0.5006 - acc: 1.0000\n",
      "Epoch 94/100\n",
      "1/1 - 0s - loss: 0.4983 - acc: 1.0000\n",
      "Epoch 95/100\n",
      "1/1 - 0s - loss: 0.4961 - acc: 1.0000\n",
      "Epoch 96/100\n",
      "1/1 - 0s - loss: 0.4938 - acc: 1.0000\n",
      "Epoch 97/100\n",
      "1/1 - 0s - loss: 0.4915 - acc: 1.0000\n",
      "Epoch 98/100\n",
      "1/1 - 0s - loss: 0.4893 - acc: 1.0000\n",
      "Epoch 99/100\n",
      "1/1 - 0s - loss: 0.4870 - acc: 1.0000\n",
      "Epoch 100/100\n",
      "1/1 - 0s - loss: 0.4848 - acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x22930c9b700>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "model.fit(X_train, y_train, epochs=100, verbose=2)\n",
    "\n",
    "# 정확도 의미가 없지만 다른 가중치들과 함께 학습된 값들 인것 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 사전 훈련된 워드 임베딩 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To\n",
      "om\n",
      "mN\n",
      "NT\n",
      "To\n",
      "om\n",
      "ms\n"
     ]
    }
   ],
   "source": [
    "### 2-gram 출력해보기\n",
    "text = 'TomNToms'\n",
    "\n",
    "for i in range(len(text) - 1): # 2-gram 이므로 문자열의 한 글자 앞까지만 반복함\n",
    "    print(text[i], text[i+1], sep='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) NLTK을 이용한 N그램"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "bigram:\n",
      "('The', 'future')\n",
      "('future', 'depends')\n",
      "('depends', 'on')\n",
      "('on', 'what')\n",
      "('what', 'we')\n",
      "('we', 'do')\n",
      "('do', 'in')\n",
      "('in', 'the')\n",
      "('the', 'present')\n",
      "\n",
      "trigram\n",
      "('The', 'future', 'depends')\n",
      "('future', 'depends', 'on')\n",
      "('depends', 'on', 'what')\n",
      "('on', 'what', 'we')\n",
      "('what', 'we', 'do')\n",
      "('we', 'do', 'in')\n",
      "('do', 'in', 'the')\n",
      "('in', 'the', 'present')\n"
     ]
    }
   ],
   "source": [
    "from nltk import bigrams, word_tokenize\n",
    "from nltk.util import ngrams\n",
    "\n",
    "sentence = \"The future depends on what we do in the present\"\n",
    "tokens = word_tokenize(sentence)\n",
    "\n",
    "bigram = bigrams(tokens)\n",
    "trigram = ngrams(tokens, 3)\n",
    "\n",
    "print(\"\\nbigram:\")\n",
    "for t in bigram:\n",
    "    print(t)\n",
    "    \n",
    "print(\"\\ntrigram\")\n",
    "for t in trigram:\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN, LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 신경망의 일종\n",
    "* 데이터 - 트레이닝 셋을 통해서 가중치를 학습시키고 테스트 셋을 통해 확인\n",
    "* 데이터의 시간적인 측면을 고려한다.\n",
    "* 은닉층의 결과가 다시 은닉층의 입력으로 들어감\n",
    "* Sequence data를 다루는데 도움이 됨.\n",
    "* ex)the clouds are in the sky 라는 문장이 있을 때 the clouds are in th 라는 문장이 있으면 뒤 단어가 sky일 확률 높음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "* RNN과 달리 상호작용을 하는 4개의 레이어 존재\n",
    "* RNN의 문제점인 입력데이터와 참고데이터의 위치 차이가 커질때 문맥 연결의 힘든점을 보완"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN(합성곱 신경망)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 이미지 처리를 위한 신경망\n",
    "* 합성곱층과 풀링층으로 구성\n",
    "* 합성곱층과 풀링층 번갈아가면서 이미지의 특징을 단계적으로 추출\n",
    "* 1D CNN모형을 통해서 문서분류 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention, Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Attention\n",
    "* 전체 또는 특정 영역의 입력값을 반영해서 그 중 어떤 부분에 집중해야 하는지 나타내줌\n",
    "* 기존의 seq2seq 모델의 단점 보완"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Transformer\n",
    "* Attention을 입력 -> 출력 외에 입력 단어들끼리도 적용하여 입력 + 출력 -> 출력으로의 attention을 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
